import os
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain_openai import ChatOpenAI  # ä½¿ç”¨æ–°ç‰ˆLangChainå¯¼å…¥æ–¹å¼

def generate_flower_recommendation(user_input: str):
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆé²œèŠ±æ¨è
    
    å‚æ•°:
        user_input: ç”¨æˆ·çš„èŠ±å‰åå¥½æè¿°
        
    è¿”å›:
        AIç”Ÿæˆçš„é²œèŠ±æ¨èç»“æœ
    """
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise ValueError("æœªæ‰¾åˆ°DEEPSEEK_API_KEYç¯å¢ƒå˜é‡")
    
    
    # åˆå§‹åŒ–DeepSeekæ¨¡å‹
    llm = ChatOpenAI(
        api_key=api_key,
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat",
        temperature=0.3
    )
    
    # è§’è‰²è®¾å®šæ¨¡æ¿
    role_template = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èŠ±åº—ç”µå•†AIåŠ©æ‰‹ï¼Œä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©å®¢æˆ·æ ¹æ®ä»–ä»¬çš„å–œå¥½åšå‡ºæ˜æ™ºçš„èŠ±å‰é€‰æ‹©"
    
    # æ€ç»´é“¾(COT)æ¨¡æ¿
    cot_template = """
## æ¨ç†è¿‡ç¨‹æŒ‡å—
æˆ‘å°†æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ€è€ƒï¼š
1. ç†è§£å®¢æˆ·çš„å…·ä½“éœ€æ±‚
2. åˆ†æèŠ±å‰çš„é¢œè‰²ã€èŠ±è¯­å’Œè±¡å¾æ„ä¹‰
3. ç»“åˆå®¢æˆ·åå¥½ç»™å‡ºä¸ªæ€§åŒ–æ¨è
4. è§£é‡Šæ¨èç†ç”±

## å‚è€ƒæ¡ˆä¾‹
æ¡ˆä¾‹1:
  å®¢æˆ·ï¼šæˆ‘æƒ³æ‰¾ä¸€ç§è±¡å¾çˆ±æƒ…çš„èŠ±ã€‚
  æ¨èï¼šçº¢ç«ç‘°
  ç†ç”±ï¼šçº¢ç«ç‘°æ˜¯çˆ±æƒ…çš„ç»å…¸è±¡å¾ï¼Œçº¢è‰²ä»£è¡¨çƒ­æƒ…å’Œæµ“çƒˆçš„æ„Ÿæƒ…ï¼Œå®Œç¾è¡¨è¾¾çˆ±æ„ã€‚

æ¡ˆä¾‹2:
  å®¢æˆ·ï¼šæˆ‘æƒ³è¦ä¸€äº›ç‹¬ç‰¹å’Œå¥‡ç‰¹çš„èŠ±ã€‚
  æ¨èï¼šå…°èŠ±
  ç†ç”±ï¼šå…°èŠ±å¤–å½¢ç‹¬ç‰¹ã€é¢œè‰²é²œè‰³ï¼Œè±¡å¾å¥¢åå’Œç‹¬ç‰¹ä¹‹ç¾ï¼Œæ»¡è¶³å¯¹ç‹¬ç‰¹æ€§çš„è¿½æ±‚ã€‚
"""
    
    # åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯æç¤ºæ¨¡æ¿
    system_prompt_role = SystemMessagePromptTemplate.from_template(role_template)
    system_prompt_cot = SystemMessagePromptTemplate.from_template(cot_template)
    
    # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯æç¤ºæ¨¡æ¿
    human_template = "å®¢æˆ·éœ€æ±‚: {human_input}"
    human_prompt = HumanMessagePromptTemplate.from_template(human_template)
    
    # ç»„åˆèŠå¤©æç¤º
    chat_prompt = ChatPromptTemplate.from_messages([
        system_prompt_role,
        system_prompt_cot,
        human_prompt
    ])
    
    # æ ¼å¼åŒ–æç¤º
    prompt = chat_prompt.format_prompt(human_input=user_input).to_messages()
    
    # æ‰“å°ç”Ÿæˆçš„æç¤ºï¼ˆè°ƒè¯•ç”¨ï¼‰
    print("=" * 60)
    print("ç”Ÿæˆçš„æç¤ºæ¶ˆæ¯:")
    for message in prompt:
        print(f"[{message.type}]: {message.content}")
    print("=" * 60)
    
    # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”
    response = llm.invoke(prompt)
    
    return response.content

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æˆ·è¾“å…¥
    user_requests = [
        "æˆ‘æƒ³ä¸ºæˆ‘çš„å¥³æœ‹å‹è´­ä¹°ä¸€äº›èŠ±ã€‚å¥¹å–œæ¬¢ç²‰è‰²å’Œç´«è‰²ã€‚ä½ æœ‰ä»€ä¹ˆå»ºè®®å—?",
        "æˆ‘éœ€è¦ä¸ºå©šç¤¼å‡†å¤‡èŠ±å‰ï¼Œæƒ³è¦ä¼˜é›…ç™½è‰²ç³»çš„èŠ±",
        "æœ‹å‹åˆšå‡èŒï¼Œæƒ³é€æœ‰æˆåŠŸå¯“æ„çš„èŠ±"
    ]
    
    for i, request in enumerate(user_requests, 1):
        print(f"\n{'=' * 30} è¯·æ±‚ #{i} {'=' * 30}")
        print(f"ğŸ‘¤ ç”¨æˆ·: {request}")
        
        # ç”Ÿæˆæ¨è
        recommendation = generate_flower_recommendation(request)
        
        print(f"\nğŸ¤– AIæ¨è:")
        print(recommendation)
        print("=" * 60)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        with open(f"flower_recommendation_{i}.txt", "w", encoding="utf-8") as f:
            f.write(recommendation)
        print(f"ç»“æœå·²ä¿å­˜åˆ° flower_recommendation_{i}.txt")